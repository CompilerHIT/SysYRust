	.text
	.align	1
	.globl	sort
    .type sort, @function
sort:
    mv x9, x10
    mv x18, x11
    addiw x19, zero, 0
	j .LBB0_1
.LBB0_1:
    mv x19, x19
    addiw x20, x18, -1
    bge    x19, x20, .LBB0_2
	j .LBB0_3
.LBB0_2:
    ret
.LBB0_3:
    addiw x20, x19, 1
    mv x20, x20
	j .LBB0_4
.LBB0_4:
    mv x20, x20
    bge    x20, x18, .LBB0_5
	j .LBB0_6
.LBB0_5:
    addiw x19, x19, 1
    mv x19, x19
	j .LBB0_1
.LBB0_6:
    slliw x21, x19, 2
    add x21, x9, x21
	lw x21, 0(x21)
    slliw x22, x20, 2
    add x22, x9, x22
	lw x22, 0(x22)
    bge    x21, x22, .LBB0_7
	j .LBB0_8
.LBB0_7:
    addiw x20, x20, 1
    mv x20, x20
	j .LBB0_4
.LBB0_8:
    slliw x21, x19, 2
    add x21, x9, x21
	lw x21, 0(x21)
    slliw x22, x20, 2
    add x22, x9, x22
	lw x22, 0(x22)
    slliw x23, x19, 2
    add x23, x9, x23
	sw x22, 0(x23)
    slliw x22, x20, 2
    add x22, x9, x22
	sw x21, 0(x22)
	j .LBB0_7
	.text
	.align	1
	.globl	param16
    .type param16, @function
param16:
	ld x9, -64(sp)
	ld x18, -56(sp)
	ld x19, -48(sp)
	ld x20, -40(sp)
	ld x21, -32(sp)
	ld x22, -24(sp)
	ld x23, -16(sp)
	ld x24, -8(sp)
    mv x25, x17
    mv x26, x16
    mv x27, x15
    mv x14, x14
    mv x13, x13
    mv x12, x12
    mv x15, x11
    mv x16, x10
    addi x17, x2, 64
	sw x16, 0(x17)
	sw x15, 4(x17)
	sw x12, 8(x17)
	sw x13, 12(x17)
	sw x14, 16(x17)
	sw x27, 20(x17)
	sw x26, 24(x17)
	sw x25, 28(x17)
	sw x24, 32(x17)
	sw x23, 36(x17)
	sw x22, 40(x17)
	sw x21, 44(x17)
	sw x20, 48(x17)
	sw x19, 52(x17)
	sw x18, 56(x17)
	sw x9, 60(x17)
    addiw x10, zero, 16
    mv x11, x10
    addiw x10, zero, 0
    slliw x10, x10, 2
    add x10, x17, x10
    mv x10, x10
	call sort
	lw x322, 0(x17)
	lw x323, 4(x17)
	lw x324, 8(x17)
	lw x325, 12(x17)
	lw x326, 16(x17)
	lw x327, 20(x17)
	lw x328, 24(x17)
	lw x10, 28(x17)
	lw x330, 32(x17)
	lw x11, 36(x17)
	lw x31, 40(x17)
	lw x28, 44(x17)
	lw x334, 48(x17)
	lw x29, 52(x17)
	lw x30, 56(x17)
	lw x17, 60(x17)
	sd x9, -200(sp)
	sd x18, -192(sp)
	sd x19, -184(sp)
	sd x20, -176(sp)
	sd x21, -168(sp)
	sd x22, -160(sp)
	sd x23, -152(sp)
	sd x24, -144(sp)
	sd x25, -136(sp)
	sd x26, -128(sp)
	sd x27, -120(sp)
	sd x14, -112(sp)
	sd x13, -104(sp)
	sd x12, -96(sp)
	sd x15, -88(sp)
	sd x16, -80(sp)
	sd x17, -72(sp)
	sd x30, -64(sp)
	sd x29, -56(sp)
	sd x334, -48(sp)
	sd x28, -40(sp)
	sd x31, -32(sp)
	sd x11, -24(sp)
	sd x330, -16(sp)
    mv x17, x10
    mv x16, x328
    mv x15, x327
    mv x14, x326
    mv x13, x325
    mv x12, x324
    mv x11, x323
    mv x10, x322
	call param32_rec
    mv x9, x10
    mv x10, x9
    ret
	.text
	.align	1
	.globl	main
    .type main, @function
main:
	call getint
    mv x9, x10
	call getint
    mv x18, x10
	call getint
    mv x19, x10
	call getint
    mv x20, x10
	call getint
    mv x21, x10
	call getint
    mv x22, x10
	call getint
    mv x23, x10
	call getint
    mv x24, x10
	call getint
    mv x25, x10
	call getint
    mv x26, x10
	call getint
    mv x27, x10
	call getint
    mv x11, x10
	call getint
    mv x12, x10
	call getint
    mv x13, x10
	call getint
    mv x14, x10
	call getint
    mv x10, x10
	sd x10, -72(sp)
	sd x14, -64(sp)
	sd x13, -56(sp)
	sd x12, -48(sp)
	sd x11, -40(sp)
	sd x27, -32(sp)
	sd x26, -24(sp)
	sd x25, -16(sp)
    mv x17, x24
    mv x16, x23
    mv x15, x22
    mv x14, x21
    mv x13, x20
    mv x12, x19
    mv x11, x18
    mv x10, x9
	call param16
    mv x9, x10
    la x18, .LC1
	sw x9, 0(x18)
    addiw x9, zero, 1
	j .LBB4_1
.LBB4_1:
    mv x9, x9
    addiw x19, zero, 32
    bge    x9, x19, .LBB4_2
	j .LBB4_3
.LBB4_2:
    addiw x9, zero, 62
    slliw x9, x9, 2
    add x9, x18, x9
	sd x9, -200(sp)
    addiw x9, zero, 60
    slliw x9, x9, 2
    add x9, x18, x9
	sd x9, -192(sp)
    addiw x9, zero, 58
    slliw x9, x9, 2
    add x9, x18, x9
	sd x9, -184(sp)
    addiw x9, zero, 56
    slliw x9, x9, 2
    add x9, x18, x9
	sd x9, -176(sp)
    addiw x9, zero, 54
    slliw x9, x9, 2
    add x9, x18, x9
	sd x9, -168(sp)
    addiw x9, zero, 52
    slliw x9, x9, 2
    add x9, x18, x9
	sd x9, -160(sp)
    addiw x9, zero, 50
    slliw x9, x9, 2
    add x9, x18, x9
	sd x9, -152(sp)
    addiw x9, zero, 48
    slliw x9, x9, 2
    add x9, x18, x9
	sd x9, -144(sp)
    addiw x9, zero, 46
    slliw x9, x9, 2
    add x9, x18, x9
	sd x9, -136(sp)
    addiw x9, zero, 44
    slliw x9, x9, 2
    add x9, x18, x9
	sd x9, -128(sp)
    addiw x9, zero, 42
    slliw x9, x9, 2
    add x9, x18, x9
	sd x9, -120(sp)
    addiw x9, zero, 40
    slliw x9, x9, 2
    add x9, x18, x9
	sd x9, -112(sp)
    addiw x9, zero, 38
    slliw x9, x9, 2
    add x9, x18, x9
	sd x9, -104(sp)
    addiw x9, zero, 36
    slliw x9, x9, 2
    add x9, x18, x9
	sd x9, -96(sp)
    addiw x9, zero, 34
    slliw x9, x9, 2
    add x9, x18, x9
	sd x9, -88(sp)
    addiw x9, zero, 32
    slliw x9, x9, 2
    add x9, x18, x9
	sd x9, -80(sp)
    addiw x9, zero, 30
    slliw x9, x9, 2
    add x9, x18, x9
	sd x9, -72(sp)
    addiw x9, zero, 28
    slliw x9, x9, 2
    add x9, x18, x9
	sd x9, -64(sp)
    addiw x9, zero, 26
    slliw x9, x9, 2
    add x9, x18, x9
	sd x9, -56(sp)
    addiw x9, zero, 24
    slliw x9, x9, 2
    add x9, x18, x9
	sd x9, -48(sp)
    addiw x9, zero, 22
    slliw x9, x9, 2
    add x9, x18, x9
	sd x9, -40(sp)
    addiw x9, zero, 20
    slliw x9, x9, 2
    add x9, x18, x9
	sd x9, -32(sp)
    addiw x9, zero, 18
    slliw x9, x9, 2
    add x9, x18, x9
	sd x9, -24(sp)
    addiw x9, zero, 16
    slliw x9, x9, 2
    add x9, x18, x9
	sd x9, -16(sp)
    addiw x9, zero, 14
    slliw x9, x9, 2
    add x9, x18, x9
    mv x17, x9
    addiw x9, zero, 12
    slliw x9, x9, 2
    add x9, x18, x9
    mv x16, x9
    addiw x9, zero, 10
    slliw x9, x9, 2
    add x9, x18, x9
    mv x15, x9
    addiw x9, zero, 8
    slliw x9, x9, 2
    add x9, x18, x9
    mv x14, x9
    addiw x9, zero, 6
    slliw x9, x9, 2
    add x9, x18, x9
    mv x13, x9
    addiw x9, zero, 4
    slliw x9, x9, 2
    add x9, x18, x9
    mv x12, x9
    addiw x9, zero, 2
    slliw x9, x9, 2
    add x9, x18, x9
    mv x11, x9
    addiw x9, zero, 0
    slliw x9, x9, 2
    add x9, x18, x9
    mv x10, x9
	call param32_arr
    mv x9, x10
    mv x10, x9
	call putint
    addiw x9, zero, 10
    mv x10, x9
	call putch
    addiw x9, zero, 0
    mv x10, x9
    ret
.LBB4_3:
    addiw x19, x9, -1
    slliw x19, x19, 1
    addiw x19, x19, 1
    slliw x19, x19, 2
    add x19, x18, x19
	lw x19, 0(x19)
    addiw x19, x19, -1
    slliw x20, x9, 1
    slliw x20, x20, 2
    add x20, x18, x20
	sw x19, 0(x20)
    addiw x19, x9, -1
    slliw x19, x19, 1
    slliw x19, x19, 2
    add x19, x18, x19
	lw x19, 0(x19)
    addiw x19, x19, -2
    slliw x20, x9, 1
    addiw x20, x20, 1
    slliw x20, x20, 2
    add x20, x18, x20
	sw x19, 0(x20)
    addiw x9, x9, 1
    mv x9, x9
	j .LBB4_1
	.text
	.align	1
	.globl	param32_arr
    .type param32_arr, @function
param32_arr:
	ld x297, -192(sp)
	ld x292, -184(sp)
	ld x287, -176(sp)
	ld x282, -168(sp)
	ld x277, -160(sp)
	ld x272, -152(sp)
	ld x267, -144(sp)
	ld x262, -136(sp)
	ld x257, -128(sp)
	ld x252, -120(sp)
	ld x27, -112(sp)
	ld x28, -104(sp)
	ld x29, -96(sp)
	ld x30, -88(sp)
	ld x31, -80(sp)
	ld x24, -72(sp)
	ld x9, -64(sp)
	ld x22, -56(sp)
	ld x18, -48(sp)
	ld x26, -40(sp)
	ld x19, -32(sp)
	ld x23, -24(sp)
	ld x20, -16(sp)
	ld x25, -8(sp)
    mv x21, x17
    mv x16, x16
    mv x15, x15
    mv x14, x14
    mv x13, x13
    mv x12, x12
    mv x11, x11
    mv x10, x10
	lw x17, 0(x10)
	lw x10, 4(x10)
    addw x10, x17, x10
	lw x17, 0(x11)
    addw x10, x10, x17
	lw x11, 4(x11)
    addw x10, x10, x11
	lw x11, 0(x12)
    addw x10, x10, x11
	lw x11, 4(x12)
    addw x10, x10, x11
	lw x11, 0(x13)
    addw x10, x10, x11
	lw x11, 4(x13)
    addw x10, x10, x11
	lw x11, 0(x14)
    addw x10, x10, x11
	lw x11, 4(x14)
    addw x10, x10, x11
	lw x11, 0(x15)
    addw x10, x10, x11
	lw x11, 4(x15)
    addw x10, x10, x11
	lw x11, 0(x16)
    addw x10, x10, x11
	lw x11, 4(x16)
    addw x10, x10, x11
	lw x11, 0(x21)
    addw x10, x10, x11
	lw x21, 4(x21)
    addw x21, x10, x21
	lw x10, 0(x25)
    addw x21, x21, x10
	lw x10, 4(x25)
    addw x21, x21, x10
	lw x10, 0(x20)
    addw x21, x21, x10
	lw x20, 4(x20)
    addw x20, x21, x20
	lw x21, 0(x23)
    addw x20, x20, x21
	lw x21, 4(x23)
    addw x20, x20, x21
	lw x21, 0(x19)
    addw x20, x20, x21
	lw x19, 4(x19)
    addw x19, x20, x19
	lw x20, 0(x26)
    addw x19, x19, x20
	lw x20, 4(x26)
    addw x19, x19, x20
	lw x20, 0(x18)
    addw x19, x19, x20
	lw x18, 4(x18)
    addw x18, x19, x18
	lw x19, 0(x22)
    addw x18, x18, x19
	lw x19, 4(x22)
    addw x18, x18, x19
	lw x19, 0(x9)
    addw x18, x18, x19
	lw x9, 4(x9)
    addw x9, x18, x9
	lw x18, 0(x24)
    addw x9, x9, x18
	lw x18, 4(x24)
    addw x9, x9, x18
	lw x18, 0(x31)
    addw x9, x9, x18
	lw x18, 4(x31)
    addw x9, x9, x18
	lw x18, 0(x30)
    addw x9, x9, x18
	lw x18, 4(x30)
    addw x9, x9, x18
	lw x18, 0(x29)
    addw x9, x9, x18
	lw x18, 4(x29)
    addw x9, x9, x18
	lw x18, 0(x28)
    addw x9, x9, x18
	lw x18, 4(x28)
    addw x9, x9, x18
	lw x18, 0(x27)
    addw x9, x9, x18
	lw x18, 4(x27)
    addw x9, x9, x18
	lw x18, 0(x252)
    addw x9, x9, x18
	lw x18, 4(x252)
    addw x9, x9, x18
	lw x18, 0(x257)
    addw x9, x9, x18
	lw x18, 4(x257)
    addw x9, x9, x18
	lw x18, 0(x262)
    addw x9, x9, x18
	lw x18, 4(x262)
    addw x9, x9, x18
	lw x18, 0(x267)
    addw x9, x9, x18
	lw x18, 4(x267)
    addw x9, x9, x18
	lw x18, 0(x272)
    addw x9, x9, x18
	lw x18, 4(x272)
    addw x9, x9, x18
	lw x18, 0(x277)
    addw x9, x9, x18
	lw x18, 4(x277)
    addw x9, x9, x18
	lw x18, 0(x282)
    addw x9, x9, x18
	lw x18, 4(x282)
    addw x9, x9, x18
	lw x18, 0(x287)
    addw x9, x9, x18
	lw x18, 4(x287)
    addw x9, x9, x18
	lw x18, 0(x292)
    addw x9, x9, x18
	lw x18, 4(x292)
    addw x9, x9, x18
	lw x18, 0(x297)
    addw x9, x9, x18
	lw x18, 4(x297)
    addw x9, x9, x18
    mv x10, x9
    ret
	.text
	.align	1
	.globl	param32_rec
    .type param32_rec, @function
param32_rec:
    mv x138, x13
    mv x137, x14
    mv x136, x15
    mv x135, x16
    mv x134, x17
	ld x22, -8(sp)
	ld x23, -16(sp)
	ld x24, -24(sp)
	ld x25, -32(sp)
	ld x26, -40(sp)
	ld x27, -48(sp)
	ld x13, -56(sp)
	ld x14, -64(sp)
	ld x15, -72(sp)
	ld x16, -80(sp)
	ld x17, -88(sp)
	ld x28, -96(sp)
	ld x29, -104(sp)
	ld x30, -112(sp)
	ld x31, -120(sp)
	ld x118, -128(sp)
	ld x9, -136(sp)
	ld x116, -144(sp)
	ld x18, -152(sp)
	ld x114, -160(sp)
	ld x19, -168(sp)
	ld x112, -176(sp)
	ld x20, -184(sp)
	ld x110, -192(sp)
    mv x21, x12
    mv x11, x11
    mv x10, x10
    addiw x12, zero, 0
    bne    x10, x12, .LBB1_1
	j .LBB1_2
.LBB1_1:
    addiw x10, x10, -1
    addw x21, x11, x21
    li x11, 288737297
    mul x11, x11, x21
    srli x11, x11, 32
    sraiw x11, x11, 26
    srliw x12, x21, 31
    addw x11, x12, x11
    li x12, 998244353
    mulw x11, x11, x12
    subw x21, x21, x11
    addiw x11, zero, 0
	sd x11, -200(sp)
	sd x110, -192(sp)
	sd x20, -184(sp)
	sd x112, -176(sp)
	sd x19, -168(sp)
	sd x114, -160(sp)
	sd x18, -152(sp)
	sd x116, -144(sp)
	sd x9, -136(sp)
	sd x118, -128(sp)
	sd x31, -120(sp)
	sd x30, -112(sp)
	sd x29, -104(sp)
	sd x28, -96(sp)
	sd x17, -88(sp)
	sd x16, -80(sp)
	sd x15, -72(sp)
	sd x14, -64(sp)
	sd x13, -56(sp)
	sd x27, -48(sp)
	sd x26, -40(sp)
	sd x25, -32(sp)
	sd x24, -24(sp)
	sd x23, -16(sp)
    mv x17, x22
    mv x16, x134
    mv x15, x135
    mv x14, x136
    mv x13, x137
    mv x12, x138
    mv x11, x21
    mv x10, x10
	call param32_rec
    mv x9, x10
    mv x9, x9
	j .LBB1_3
.LBB1_2:
    mv x9, x11
	j .LBB1_3
.LBB1_3:
    mv x9, x9
    mv x10, x9
    ret