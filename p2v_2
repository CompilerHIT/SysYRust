	.text
	.align	1
	.globl	sort
    .type sort, @function
sort:
    mv x485, x10
    mv x484, x11
    addiw x483, zero, 0
	j .LBB0_1
.LBB0_1:
    mv x488, x483
    addiw x489, x484, -1
    bge    x488, x489, .LBB0_2
	j .LBB0_3
.LBB0_2:
    ret
.LBB0_3:
    addiw x491, x488, 1
    mv x490, x491
	j .LBB0_4
.LBB0_4:
    mv x492, x490
    bge    x492, x484, .LBB0_5
	j .LBB0_6
.LBB0_5:
    addiw x493, x488, 1
    mv x483, x493
	j .LBB0_1
.LBB0_6:
    slliw x499, x488, 2
    add x498, x485, x499
	lw x494, 0(x498)
    slliw x497, x492, 2
    add x496, x485, x497
	lw x495, 0(x496)
    bge    x494, x495, .LBB0_7
	j .LBB0_8
.LBB0_7:
    addiw x500, x492, 1
    mv x490, x500
	j .LBB0_4
.LBB0_8:
    slliw x510, x488, 2
    add x509, x485, x510
	lw x502, 0(x509)
    slliw x508, x492, 2
    add x507, x485, x508
	lw x505, 0(x507)
    slliw x506, x488, 2
    add x504, x485, x506
	sw x505, 0(x504)
    slliw x503, x492, 2
    add x501, x485, x503
	sw x502, 0(x501)
	j .LBB0_7
	.text
	.align	1
	.globl	param16
    .type param16, @function
param16:
	ld x552, -64(sp)
	ld x551, -56(sp)
	ld x550, -48(sp)
	ld x549, -40(sp)
	ld x548, -32(sp)
	ld x547, -24(sp)
	ld x546, -16(sp)
	ld x545, -8(sp)
    mv x544, x17
    mv x543, x16
    mv x542, x15
    mv x541, x14
    mv x540, x13
    mv x539, x12
    mv x538, x11
    mv x537, x10
    addi x553, x2, 64
	sw x537, 0(x553)
	sw x538, 4(x553)
	sw x539, 8(x553)
	sw x540, 12(x553)
	sw x541, 16(x553)
	sw x542, 20(x553)
	sw x543, 24(x553)
	sw x544, 28(x553)
	sw x545, 32(x553)
	sw x546, 36(x553)
	sw x547, 40(x553)
	sw x548, 44(x553)
	sw x549, 48(x553)
	sw x550, 52(x553)
	sw x551, 56(x553)
	sw x552, 60(x553)
    addiw x559, zero, 16
    mv x11, x559
    addiw x558, zero, 0
    slliw x557, x558, 2
    add x556, x553, x557
    mv x10, x556
	call sort
	lw x322, 0(x553)
	lw x323, 4(x553)
	lw x324, 8(x553)
	lw x325, 12(x553)
	lw x326, 16(x553)
	lw x327, 20(x553)
	lw x328, 24(x553)
	lw x530, 28(x553)
	lw x330, 32(x553)
	lw x531, 36(x553)
	lw x532, 40(x553)
	lw x533, 44(x553)
	lw x334, 48(x553)
	lw x534, 52(x553)
	lw x535, 56(x553)
	lw x536, 60(x553)
	sd x552, -200(sp)
	sd x551, -192(sp)
	sd x550, -184(sp)
	sd x549, -176(sp)
	sd x548, -168(sp)
	sd x547, -160(sp)
	sd x546, -152(sp)
	sd x545, -144(sp)
	sd x544, -136(sp)
	sd x543, -128(sp)
	sd x542, -120(sp)
	sd x541, -112(sp)
	sd x540, -104(sp)
	sd x539, -96(sp)
	sd x538, -88(sp)
	sd x537, -80(sp)
	sd x536, -72(sp)
	sd x535, -64(sp)
	sd x534, -56(sp)
	sd x334, -48(sp)
	sd x533, -40(sp)
	sd x532, -32(sp)
	sd x531, -24(sp)
	sd x330, -16(sp)
    mv x17, x530
    mv x16, x328
    mv x15, x327
    mv x14, x326
    mv x13, x325
    mv x12, x324
    mv x11, x323
    mv x10, x322
	call param32_rec
    mv x520, x10
    mv x10, x520
    ret
	.text
	.align	1
	.globl	main
    .type main, @function
main:
	call getint
    mv x580, x10
	call getint
    mv x581, x10
	call getint
    mv x582, x10
	call getint
    mv x583, x10
	call getint
    mv x584, x10
	call getint
    mv x585, x10
	call getint
    mv x586, x10
	call getint
    mv x587, x10
	call getint
    mv x588, x10
	call getint
    mv x589, x10
	call getint
    mv x590, x10
	call getint
    mv x591, x10
	call getint
    mv x592, x10
	call getint
    mv x593, x10
	call getint
    mv x594, x10
	call getint
    mv x595, x10
	sd x595, -72(sp)
	sd x594, -64(sp)
	sd x593, -56(sp)
	sd x592, -48(sp)
	sd x591, -40(sp)
	sd x590, -32(sp)
	sd x589, -24(sp)
	sd x588, -16(sp)
    mv x17, x587
    mv x16, x586
    mv x15, x585
    mv x14, x584
    mv x13, x583
    mv x12, x582
    mv x11, x581
    mv x10, x580
	call param16
    mv x570, x10
    la x568, .LC1
	sw x570, 0(x568)
    addiw x569, zero, 1
	j .LBB4_1
.LBB4_1:
    mv x612, x569
    addiw x613, zero, 32
    bge    x612, x613, .LBB4_2
	j .LBB4_3
.LBB4_2:
    addiw x724, zero, 62
    slliw x723, x724, 2
    add x722, x568, x723
	sd x722, -200(sp)
    addiw x721, zero, 60
    slliw x720, x721, 2
    add x719, x568, x720
	sd x719, -192(sp)
    addiw x718, zero, 58
    slliw x717, x718, 2
    add x716, x568, x717
	sd x716, -184(sp)
    addiw x715, zero, 56
    slliw x714, x715, 2
    add x713, x568, x714
	sd x713, -176(sp)
    addiw x712, zero, 54
    slliw x711, x712, 2
    add x710, x568, x711
	sd x710, -168(sp)
    addiw x709, zero, 52
    slliw x708, x709, 2
    add x707, x568, x708
	sd x707, -160(sp)
    addiw x706, zero, 50
    slliw x705, x706, 2
    add x704, x568, x705
	sd x704, -152(sp)
    addiw x703, zero, 48
    slliw x702, x703, 2
    add x701, x568, x702
	sd x701, -144(sp)
    addiw x700, zero, 46
    slliw x699, x700, 2
    add x698, x568, x699
	sd x698, -136(sp)
    addiw x697, zero, 44
    slliw x696, x697, 2
    add x695, x568, x696
	sd x695, -128(sp)
    addiw x694, zero, 42
    slliw x693, x694, 2
    add x692, x568, x693
	sd x692, -120(sp)
    addiw x691, zero, 40
    slliw x690, x691, 2
    add x689, x568, x690
	sd x689, -112(sp)
    addiw x688, zero, 38
    slliw x687, x688, 2
    add x686, x568, x687
	sd x686, -104(sp)
    addiw x685, zero, 36
    slliw x684, x685, 2
    add x683, x568, x684
	sd x683, -96(sp)
    addiw x682, zero, 34
    slliw x681, x682, 2
    add x680, x568, x681
	sd x680, -88(sp)
    addiw x679, zero, 32
    slliw x678, x679, 2
    add x677, x568, x678
	sd x677, -80(sp)
    addiw x676, zero, 30
    slliw x675, x676, 2
    add x674, x568, x675
	sd x674, -72(sp)
    addiw x673, zero, 28
    slliw x672, x673, 2
    add x671, x568, x672
	sd x671, -64(sp)
    addiw x670, zero, 26
    slliw x669, x670, 2
    add x668, x568, x669
	sd x668, -56(sp)
    addiw x667, zero, 24
    slliw x666, x667, 2
    add x665, x568, x666
	sd x665, -48(sp)
    addiw x664, zero, 22
    slliw x663, x664, 2
    add x662, x568, x663
	sd x662, -40(sp)
    addiw x661, zero, 20
    slliw x660, x661, 2
    add x659, x568, x660
	sd x659, -32(sp)
    addiw x658, zero, 18
    slliw x657, x658, 2
    add x656, x568, x657
	sd x656, -24(sp)
    addiw x655, zero, 16
    slliw x654, x655, 2
    add x653, x568, x654
	sd x653, -16(sp)
    addiw x652, zero, 14
    slliw x651, x652, 2
    add x650, x568, x651
    mv x17, x650
    addiw x649, zero, 12
    slliw x648, x649, 2
    add x647, x568, x648
    mv x16, x647
    addiw x646, zero, 10
    slliw x645, x646, 2
    add x644, x568, x645
    mv x15, x644
    addiw x643, zero, 8
    slliw x642, x643, 2
    add x641, x568, x642
    mv x14, x641
    addiw x640, zero, 6
    slliw x639, x640, 2
    add x638, x568, x639
    mv x13, x638
    addiw x637, zero, 4
    slliw x636, x637, 2
    add x635, x568, x636
    mv x12, x635
    addiw x634, zero, 2
    slliw x633, x634, 2
    add x632, x568, x633
    mv x11, x632
    addiw x631, zero, 0
    slliw x630, x631, 2
    add x629, x568, x630
    mv x10, x629
	call param32_arr
    mv x619, x10
    mv x10, x619
	call putint
    addiw x617, zero, 10
    mv x10, x617
	call putch
    addiw x615, zero, 0
    mv x10, x615
    ret
.LBB4_3:
    addiw x745, x612, -1
    slliw x744, x745, 1
    addiw x743, x744, 1
    slliw x742, x743, 2
    add x741, x568, x742
	lw x740, 0(x741)
    addiw x737, x740, -1
    slliw x739, x612, 1
    slliw x738, x739, 2
    add x736, x568, x738
	sw x737, 0(x736)
    addiw x735, x612, -1
    slliw x734, x735, 1
    slliw x733, x734, 2
    add x732, x568, x733
	lw x731, 0(x732)
    addiw x727, x731, -2
    slliw x730, x612, 1
    addiw x729, x730, 1
    slliw x728, x729, 2
    add x726, x568, x728
	sw x727, 0(x726)
    addiw x725, x612, 1
    mv x569, x725
	j .LBB4_1
	.text
	.align	1
	.globl	param32_arr
    .type param32_arr, @function
param32_arr:
	ld x297, -192(sp)
	ld x292, -184(sp)
	ld x287, -176(sp)
	ld x282, -168(sp)
	ld x277, -160(sp)
	ld x272, -152(sp)
	ld x267, -144(sp)
	ld x262, -136(sp)
	ld x257, -128(sp)
	ld x252, -120(sp)
	ld x798, -112(sp)
	ld x803, -104(sp)
	ld x808, -96(sp)
	ld x813, -88(sp)
	ld x818, -80(sp)
	ld x823, -72(sp)
	ld x828, -64(sp)
	ld x833, -56(sp)
	ld x838, -48(sp)
	ld x843, -40(sp)
	ld x848, -32(sp)
	ld x853, -24(sp)
	ld x858, -16(sp)
	ld x863, -8(sp)
    mv x868, x17
    mv x873, x16
    mv x878, x15
    mv x883, x14
    mv x888, x13
    mv x893, x12
    mv x898, x11
    mv x903, x10
	lw x901, 0(x903)
	lw x902, 4(x903)
    addw x899, x901, x902
	lw x900, 0(x898)
    addw x896, x899, x900
	lw x897, 4(x898)
    addw x894, x896, x897
	lw x895, 0(x893)
    addw x891, x894, x895
	lw x892, 4(x893)
    addw x889, x891, x892
	lw x890, 0(x888)
    addw x886, x889, x890
	lw x887, 4(x888)
    addw x884, x886, x887
	lw x885, 0(x883)
    addw x881, x884, x885
	lw x882, 4(x883)
    addw x879, x881, x882
	lw x880, 0(x878)
    addw x876, x879, x880
	lw x877, 4(x878)
    addw x874, x876, x877
	lw x875, 0(x873)
    addw x871, x874, x875
	lw x872, 4(x873)
    addw x869, x871, x872
	lw x870, 0(x868)
    addw x866, x869, x870
	lw x867, 4(x868)
    addw x864, x866, x867
	lw x865, 0(x863)
    addw x861, x864, x865
	lw x862, 4(x863)
    addw x859, x861, x862
	lw x860, 0(x858)
    addw x856, x859, x860
	lw x857, 4(x858)
    addw x854, x856, x857
	lw x855, 0(x853)
    addw x851, x854, x855
	lw x852, 4(x853)
    addw x849, x851, x852
	lw x850, 0(x848)
    addw x846, x849, x850
	lw x847, 4(x848)
    addw x844, x846, x847
	lw x845, 0(x843)
    addw x841, x844, x845
	lw x842, 4(x843)
    addw x839, x841, x842
	lw x840, 0(x838)
    addw x836, x839, x840
	lw x837, 4(x838)
    addw x834, x836, x837
	lw x835, 0(x833)
    addw x831, x834, x835
	lw x832, 4(x833)
    addw x829, x831, x832
	lw x830, 0(x828)
    addw x826, x829, x830
	lw x827, 4(x828)
    addw x824, x826, x827
	lw x825, 0(x823)
    addw x821, x824, x825
	lw x822, 4(x823)
    addw x819, x821, x822
	lw x820, 0(x818)
    addw x816, x819, x820
	lw x817, 4(x818)
    addw x814, x816, x817
	lw x815, 0(x813)
    addw x811, x814, x815
	lw x812, 4(x813)
    addw x809, x811, x812
	lw x810, 0(x808)
    addw x806, x809, x810
	lw x807, 4(x808)
    addw x804, x806, x807
	lw x805, 0(x803)
    addw x801, x804, x805
	lw x802, 4(x803)
    addw x799, x801, x802
	lw x800, 0(x798)
    addw x796, x799, x800
	lw x797, 4(x798)
    addw x794, x796, x797
	lw x795, 0(x252)
    addw x792, x794, x795
	lw x793, 4(x252)
    addw x790, x792, x793
	lw x791, 0(x257)
    addw x788, x790, x791
	lw x789, 4(x257)
    addw x786, x788, x789
	lw x787, 0(x262)
    addw x784, x786, x787
	lw x785, 4(x262)
    addw x782, x784, x785
	lw x783, 0(x267)
    addw x780, x782, x783
	lw x781, 4(x267)
    addw x778, x780, x781
	lw x779, 0(x272)
    addw x776, x778, x779
	lw x777, 4(x272)
    addw x774, x776, x777
	lw x775, 0(x277)
    addw x772, x774, x775
	lw x773, 4(x277)
    addw x770, x772, x773
	lw x771, 0(x282)
    addw x768, x770, x771
	lw x769, 4(x282)
    addw x766, x768, x769
	lw x767, 0(x287)
    addw x764, x766, x767
	lw x765, 4(x287)
    addw x762, x764, x765
	lw x763, 0(x292)
    addw x760, x762, x763
	lw x761, 4(x292)
    addw x758, x760, x761
	lw x759, 0(x297)
    addw x756, x758, x759
	lw x757, 4(x297)
    addw x755, x756, x757
    mv x10, x755
    ret
	.text
	.align	1
	.globl	param32_rec
    .type param32_rec, @function
param32_rec:
    mv x138, x924
    mv x137, x936
    mv x136, x935
    mv x135, x921
    mv x134, x939
	ld x931, -8(sp)
	ld x929, -16(sp)
	ld x932, -24(sp)
	ld x938, -32(sp)
	ld x941, -40(sp)
	ld x930, -48(sp)
	ld x924, -56(sp)
	ld x936, -64(sp)
	ld x935, -72(sp)
	ld x921, -80(sp)
	ld x939, -88(sp)
	ld x928, -96(sp)
	ld x922, -104(sp)
	ld x920, -112(sp)
	ld x925, -120(sp)
	ld x118, -128(sp)
	ld x923, -136(sp)
	ld x116, -144(sp)
	ld x940, -152(sp)
	ld x114, -160(sp)
	ld x926, -168(sp)
	ld x112, -176(sp)
	ld x934, -184(sp)
	ld x110, -192(sp)
    mv x937, x12
    mv x927, x927
    mv x933, x933
    addiw x942, zero, 0
    bne    x933, x942, .LBB1_1
	j .LBB1_2
.LBB1_1:
    addiw x962, x933, -1
    addw x965, x927, x937
    li x973, 288737297
    mul x972, x973, x965
    srli x971, x972, 32
    sraiw x970, x971, 26
    srliw x969, x965, 31
    addw x967, x969, x970
    li x968, 998244353
    mulw x966, x967, x968
    subw x963, x965, x966
    addiw x964, zero, 0
	sd x964, -200(sp)
	sd x110, -192(sp)
	sd x934, -184(sp)
	sd x112, -176(sp)
	sd x926, -168(sp)
	sd x114, -160(sp)
	sd x940, -152(sp)
	sd x116, -144(sp)
	sd x923, -136(sp)
	sd x118, -128(sp)
	sd x925, -120(sp)
	sd x920, -112(sp)
	sd x922, -104(sp)
	sd x928, -96(sp)
	sd x939, -88(sp)
	sd x921, -80(sp)
	sd x935, -72(sp)
	sd x936, -64(sp)
	sd x924, -56(sp)
	sd x930, -48(sp)
	sd x941, -40(sp)
	sd x938, -32(sp)
	sd x932, -24(sp)
	sd x929, -16(sp)
    mv x17, x931
    mv x16, x134
    mv x15, x135
    mv x14, x136
    mv x13, x137
    mv x12, x138
    mv x11, x963
    mv x10, x962
	call param32_rec
    mv x952, x10
    mv x951, x952
	j .LBB1_3
.LBB1_2:
    mv x951, x927
	j .LBB1_3
.LBB1_3:
    mv x975, x951
    mv x10, x975
    ret